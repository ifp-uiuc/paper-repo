# saxe_2014_iclr_exact

http://arxiv.org/pdf/1312.6120

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

# Tags
+ linear
+ optimization
+ orthogonal

# Description  
The main take away for me: orthogonal initializations (on the edge of chaos) help propagate the gradient in deep nonlinear neural networks.